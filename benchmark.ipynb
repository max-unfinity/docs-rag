{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls_...\"  # Your API key\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "\n",
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry\n",
    "langchain_docs = registry[\"LangChain Docs Q&A\"]\n",
    "langchain_docs\n",
    "docs = list(langchain_docs.get_docs())\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"thenlper/gte-base\",\n",
    "#     # model_kwargs={\"device\": 0},  # Comment out to use CPU\n",
    "# )\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"lcbm-b-huggingface-gte-base\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=\"./chromadb\",\n",
    "# )\n",
    "\n",
    "# vectorstore.add_documents(docs)\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-base\",\n",
    "    # model_kwargs={\"device\": 0},  # Comment out to use CPU\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"lcbm-b-huggingface-gte-base\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chromadb\",\n",
    ")\n",
    "\n",
    "# vectorstore.add_documents(docs)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Chroma | ü¶úÔ∏èüîó Langchain\\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)# Chroma\\n\\n[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.\\n\\nInstall Chroma with:\\n\\n```sh\\npip install chromadb\\n```\\n\\nChroma runs in various modes. See below for examples of each integrated with LangChain.\\n\\n- `in-memory` - in a python script or jupyter notebook\\n\\n- `in-memory with persistance` - in a script or notebook and save/load to disk\\n\\n- `in a docker container` - as a server running your local machine or in the cloud\\n\\nLike any other database, you can: \\n\\n- `.add` \\n\\n- `.get` \\n\\n- `.update`\\n\\n- `.upsert`\\n\\n- `.delete`\\n\\n- `.peek`\\n\\n- and `.query` runs the similarity search.\\n\\nView full docs at [docs](https://docs.trychroma.com/reference/Collection). To access these methods directly, you can do `._collection.method()`\\n\\n## Basic Example\\u200b\\n\\nIn this basic example, we take the most recent State of the Union Address, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it.\\n\\n```python\\n# import\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\n\\n# load the document and split it into chunks\\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\\ndocuments = loader.load()\\n\\n# split it into chunks\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocs = text_splitter.split_documents(documents)\\n\\n# create the open-source embedding function\\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n\\n# load it into Chroma\\ndb = Chroma.from_documents(docs, embedding_function)\\n\\n# query it\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = db.similarity_search(query)\\n\\n# print results\\nprint(docs[0].page_content)\\n```\\n\\n```text\\n    /Users/jeff/.pyenv/versions/3.10.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\n      from .autonotebook import tqdm as notebook_tqdm\\n\\n    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n    \\n    Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n    \\n    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n    \\n    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\n```\\n\\n## Basic Example (including saving to disk)\\u200b\\n\\nExtending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to. \\n\\n`Caution`: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other\\'s work. As a best practice, only have one client per path running at any given time.\\n\\n```python\\n# save to disk\\ndb2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\\ndocs = db2.similarity_search(query)\\n\\n# load from disk\\ndb3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\\ndocs = db3.similarity_search(query)\\nprint(docs[0].page_content)\\n```\\n\\n```text\\n    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n    \\n    Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n    \\n    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n    \\n    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\n```\\n\\n## Passing a Chroma Client into Langchain\\u200b\\n\\nYou can also create a Chroma Client and pass it to LangChain. This is particularly useful if you want easier access to the underlying database.\\n\\nYou can also specify the collection name that you want LangChain to use.\\n\\n```python\\nimport chromadb\\n\\npersistent_client = chromadb.PersistentClient()\\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\\n\\nlangchain_chroma = Chroma(\\n    client=persistent_client,\\n    collection_name=\"collection_name\",\\n    embedding_function=embedding_function,\\n)\\n\\nprint(\"There are\", langchain_chroma._collection.count(), \"in the collection\")\\n```\\n\\n```text\\n    Add of existing embedding ID: 1\\n    Add of existing embedding ID: 2\\n    Add of existing embedding ID: 3\\n    Add of existing embedding ID: 1\\n    Add of existing embedding ID: 2\\n    Add of existing embedding ID: 3\\n    Add of existing embedding ID: 1\\n    Insert of existing embedding ID: 1\\n    Add of existing embedding ID: 2\\n    Insert of existing embedding ID: 2\\n    Add of existing embedding ID: 3\\n    Insert of existing embedding ID: 3\\n\\n    There are 3 in the collection\\n```\\n\\n## Basic Example (using the Docker Container)\\u200b\\n\\nYou can also run the Chroma Server in a Docker container separately, create a Client to connect to it, and then pass that to LangChain. \\n\\nChroma has the ability to handle multiple `Collections` of documents, but the LangChain interface expects one, so we need to specify the collection name. The default collection name used by LangChain is \"langchain\".\\n\\nHere is how to clone, build, and run the Docker Image:\\n\\n```sh\\ngit clone git@github.com:chroma-core/chroma.git\\n```\\n\\nEdit the `docker-compose.yml` file and add `ALLOW_RESET=TRUE` under `environment`\\n\\n```yaml\\n    ...\\n    command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config log_config.yml\\n    environment:\\n      - IS_PERSISTENT=TRUE\\n      - ALLOW_RESET=TRUE\\n    ports:\\n      - 8000:8000\\n    ...\\n```\\n\\nThen run `docker-compose up -d --build`\\n\\n```python\\n# create the chroma client\\nimport uuid\\n\\nimport chromadb\\nfrom chromadb.config import Settings\\n\\nclient = chromadb.HttpClient(settings=Settings(allow_reset=True))\\nclient.reset()  # resets the database\\ncollection = client.create_collection(\"my_collection\")\\nfor doc in docs:\\n    collection.add(\\n        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\\n    )\\n\\n# tell LangChain to use our client and collection name\\ndb4 = Chroma(\\n    client=client,\\n    collection_name=\"my_collection\",\\n    embedding_function=embedding_function,\\n)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = db4.similarity_search(query)\\nprint(docs[0].page_content)\\n```\\n\\n```text\\n    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n    \\n    Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n    \\n    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n    \\n    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\n```\\n\\n## Update and Delete\\u200b\\n\\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data. \\n\\nChroma has users provide `ids` to simplify the bookkeeping here. `ids` can be the name of the file, or a combined has like `filename_paragraphNumber`, etc.\\n\\nChroma supports all these operations - though some of them are still being integrated all the way through the LangChain interface. Additional workflow improvements will be added soon.\\n\\nHere is a basic example showing how to do various operations:\\n\\n```python\\n# create simple ids\\nids = [str(i) for i in range(1, len(docs) + 1)]\\n\\n# add data\\nexample_db = Chroma.from_documents(docs, embedding_function, ids=ids)\\ndocs = example_db.similarity_search(query)\\nprint(docs[0].metadata)\\n\\n# update the metadata for a document\\ndocs[0].metadata = {\\n    \"source\": \"../../modules/state_of_the_union.txt\",\\n    \"new_value\": \"hello world\",\\n}\\nexample_db.update_document(ids[0], docs[0])\\nprint(example_db._collection.get(ids=[ids[0]]))\\n\\n# delete the last document\\nprint(\"count before\", example_db._collection.count())\\nexample_db._collection.delete(ids=[ids[-1]])\\nprint(\"count after\", example_db._collection.count())\\n```\\n\\n```text\\n    {\\'source\\': \\'../../../state_of_the_union.txt\\'}\\n    {\\'ids\\': [\\'1\\'], \\'embeddings\\': None, \\'metadatas\\': [{\\'new_value\\': \\'hello world\\', \\'source\\': \\'../../../state_of_the_union.txt\\'}], \\'documents\\': [\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\']}\\n    count before 46\\n    count after 45\\n```\\n\\n## Use OpenAI Embeddings\\u200b\\n\\nMany people like to use OpenAIEmbeddings, here is how to set that up.\\n\\n```python\\n# get a token: https://platform.openai.com/account/api-keys\\n\\nfrom getpass import getpass\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nOPENAI_API_KEY = getpass()\\n```\\n\\n```python\\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n```\\n\\n```python\\nembeddings = OpenAIEmbeddings()\\nnew_client = chromadb.EphemeralClient()\\nopenai_lc_client = Chroma.from_documents(\\n    docs, embeddings, client=new_client, collection_name=\"openai_collection\"\\n)\\n\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = openai_lc_client.similarity_search(query)\\nprint(docs[0].page_content)\\n```\\n\\n```text\\n    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n    \\n    Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n    \\n    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n    \\n    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\n```\\n\\n## Other Information\\u200b\\n\\n### Similarity search with score\\u200b\\n\\nThe returned distance score is cosine distance. Therefore, a lower score is better.\\n\\n```python\\ndocs = db.similarity_search_with_score(query)\\n```\\n\\n```python\\ndocs[0]\\n```\\n\\n```text\\n    (Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'}),\\n     1.1972057819366455)\\n```\\n\\n### Retriever options\\u200b\\n\\nThis section goes over different options for how to use Chroma as a retriever.\\n\\n#### MMR\\u200b\\n\\nIn addition to using similarity search in the retriever object, you can also use `mmr`.\\n\\n```python\\nretriever = db.as_retriever(search_type=\"mmr\")\\n```\\n\\n```python\\nretriever.get_relevant_documents(query)[0]\\n```\\n\\n```text\\n    Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'})\\n```\\n\\n### Filtering on metadata\\u200b\\n\\nIt can be helpful to narrow down the collection before working with it.\\n\\nFor example, collections can be filtered on metadata using the get method.\\n\\n```python\\n# filter collection for updated source\\nexample_db.get(where={\"source\": \"some_other_source\"})\\n```\\n\\n```text\\n    {\\'ids\\': [], \\'embeddings\\': None, \\'metadatas\\': [], \\'documents\\': []}\\n```\\n\\n- [Basic Example](#basic-example)\\n\\n- [Basic Example (including saving to disk)](#basic-example-including-saving-to-disk)\\n\\n- [Passing a Chroma Client into Langchain](#passing-a-chroma-client-into-langchain)\\n\\n- [Basic Example (using the Docker Container)](#basic-example-using-the-docker-container)\\n\\n- [Update and Delete](#update-and-delete)\\n\\n- [Use OpenAI Embeddings](#use-openai-embeddings)\\n\\n- [Other Information](#other-information)- [Similarity search with score](#similarity-search-with-score)\\n\\n- [Retriever options](#retriever-options)\\n\\n- [Filtering on metadata](#filtering-on-metadata)', metadata={'changefreq': 'weekly', 'description': 'Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.', 'language': 'en', 'loc': 'https://python.langchain.com/docs/integrations/vectorstores/chroma', 'priority': '0.5', 'source': 'https://python.langchain.com/docs/integrations/vectorstores/chroma', 'title': 'Chroma | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='Chroma | ü¶úÔ∏èüîó Langchain\\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)# Chroma\\n\\n[Chroma](https://docs.trychroma.com/getting-started) is a database for building AI applications with embeddings.\\n\\n## Installation and Setup\\u200b\\n\\n```bash\\npip install chromadb\\n```\\n\\n## VectorStore\\u200b\\n\\nThere exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\n```python\\nfrom langchain.vectorstores import Chroma\\n```\\n\\nFor a more detailed walkthrough of the Chroma wrapper, see [this notebook](/docs/integrations/vectorstores/chroma)\\n\\n## Retriever\\u200b\\n\\nSee a [usage example](/docs/modules/data_connection/retrievers/how_to/self_query/chroma_self_query).\\n\\n```python\\nfrom langchain.retrievers import SelfQueryRetriever\\n```\\n\\n- [Installation and Setup](#installation-and-setup)\\n\\n- [VectorStore](#vectorstore)\\n\\n- [Retriever](#retriever)', metadata={'changefreq': 'weekly', 'description': 'Chroma is a database for building AI applications with embeddings.', 'language': 'en', 'loc': 'https://python.langchain.com/docs/integrations/providers/chroma', 'priority': '0.5', 'source': 'https://python.langchain.com/docs/integrations/providers/chroma', 'title': 'Chroma | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='Chroma | ü¶úÔ∏èüîó Langchain\\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)# Chroma\\n\\n[Chroma](https://docs.trychroma.com/getting-started) is a database for building AI applications with embeddings.\\n\\nIn the notebook, we\\'ll demo the `SelfQueryRetriever` wrapped around a `Chroma` vector store. \\n\\n## Creating a Chroma vector store\\u200b\\n\\nFirst we\\'ll want to create a Chroma vector store and seed it with some data. We\\'ve created a small demo set of documents that contain summaries of movies.\\n\\n**Note:** The self-query retriever requires you to have `lark` installed (`pip install lark`). We also need the `chromadb` package.\\n\\n```python\\n#!pip install lark\\n```\\n\\n```python\\n#!pip install chromadb\\n```\\n\\nWe want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key.\\n\\n```python\\nimport getpass\\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n```\\n\\n```text\\n    OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\n```\\n\\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.schema import Document\\nfrom langchain.vectorstores import Chroma\\n\\nembeddings = OpenAIEmbeddings()\\n```\\n\\n```python\\ndocs = [\\n    Document(\\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\\n    ),\\n    Document(\\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\\n    ),\\n    Document(\\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\\n    ),\\n    Document(\\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\\n    ),\\n    Document(\\n        page_content=\"Toys come alive and have a blast doing so\",\\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\\n    ),\\n    Document(\\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\\n        metadata={\\n            \"year\": 1979,\\n            \"director\": \"Andrei Tarkovsky\",\\n            \"genre\": \"science fiction\",\\n            \"rating\": 9.9,\\n        },\\n    ),\\n]\\nvectorstore = Chroma.from_documents(docs, embeddings)\\n```\\n\\n```text\\n    Using embedded DuckDB without persistence: data will be transient\\n```\\n\\n## Creating our self-querying retriever\\u200b\\n\\nNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\\n\\n```python\\nfrom langchain.chains.query_constructor.base import AttributeInfo\\nfrom langchain.llms import OpenAI\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\n\\nmetadata_field_info = [\\n    AttributeInfo(\\n        name=\"genre\",\\n        description=\"The genre of the movie\",\\n        type=\"string or list[string]\",\\n    ),\\n    AttributeInfo(\\n        name=\"year\",\\n        description=\"The year the movie was released\",\\n        type=\"integer\",\\n    ),\\n    AttributeInfo(\\n        name=\"director\",\\n        description=\"The name of the movie director\",\\n        type=\"string\",\\n    ),\\n    AttributeInfo(\\n        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\\n    ),\\n]\\ndocument_content_description = \"Brief summary of a movie\"\\nllm = OpenAI(temperature=0)\\nretriever = SelfQueryRetriever.from_llm(\\n    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\\n)\\n```\\n\\n## Testing it out\\u200b\\n\\nAnd now we can try actually using our retriever!\\n\\n```python\\n# This example only specifies a relevant query\\nretriever.get_relevant_documents(\"What are some movies about dinosaurs\")\\n```\\n\\n```text\\n    query=\\'dinosaur\\' filter=None\\n\\n    [Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'year\\': 1993, \\'rating\\': 7.7, \\'genre\\': \\'science fiction\\'}),\\n     Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'year\\': 1995, \\'genre\\': \\'animated\\'}),\\n     Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'year\\': 2006, \\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6}),\\n     Document(page_content=\\'Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\\', metadata={\\'year\\': 2010, \\'director\\': \\'Christopher Nolan\\', \\'rating\\': 8.2})]\\n```\\n\\n```python\\n# This example only specifies a filter\\nretriever.get_relevant_documents(\"I want to watch a movie rated higher than 8.5\")\\n```\\n\\n```text\\n    query=\\' \\' filter=Comparison(comparator=<Comparator.GT: \\'gt\\'>, attribute=\\'rating\\', value=8.5)\\n\\n    [Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'year\\': 2006, \\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6}),\\n     Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'year\\': 1979, \\'rating\\': 9.9, \\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'science fiction\\'})]\\n```\\n\\n```python\\n# This example specifies a query and a filter\\nretriever.get_relevant_documents(\"Has Greta Gerwig directed any movies about women\")\\n```\\n\\n```text\\n    query=\\'women\\' filter=Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Greta Gerwig\\')\\n\\n    [Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'year\\': 2019, \\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3})]\\n```\\n\\n```python\\n# This example specifies a composite filter\\nretriever.get_relevant_documents(\\n    \"What\\'s a highly rated (above 8.5) science fiction film?\"\\n)\\n```\\n\\n```text\\n    query=\\' \\' filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Comparison(comparator=<Comparator.GT: \\'gt\\'>, attribute=\\'rating\\', value=8.5)])\\n\\n    [Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'year\\': 1979, \\'rating\\': 9.9, \\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'science fiction\\'})]\\n```\\n\\n```python\\n# This example specifies a query and composite filter\\nretriever.get_relevant_documents(\\n    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\"\\n)\\n```\\n\\n```text\\n    query=\\'toys\\' filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GT: \\'gt\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2005), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'animated\\')])\\n\\n    [Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'year\\': 1995, \\'genre\\': \\'animated\\'})]\\n```\\n\\n## Filter k\\u200b\\n\\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\\n\\nWe can do this by passing `enable_limit=True` to the constructor.\\n\\n```python\\nretriever = SelfQueryRetriever.from_llm(\\n    llm,\\n    vectorstore,\\n    document_content_description,\\n    metadata_field_info,\\n    enable_limit=True,\\n    verbose=True,\\n)\\n```\\n\\n```python\\n# This example only specifies a relevant query\\nretriever.get_relevant_documents(\"what are two movies about dinosaurs\")\\n```\\n\\n```text\\n    query=\\'dinosaur\\' filter=None\\n\\n    [Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'year\\': 1993, \\'rating\\': 7.7, \\'genre\\': \\'science fiction\\'}),\\n     Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'year\\': 1995, \\'genre\\': \\'animated\\'}),\\n     Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'year\\': 2006, \\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6}),\\n     Document(page_content=\\'Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\\', metadata={\\'year\\': 2010, \\'director\\': \\'Christopher Nolan\\', \\'rating\\': 8.2})]\\n```\\n\\n- [Creating a Chroma vector store](#creating-a-chroma-vector-store)\\n\\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\\n\\n- [Testing it out](#testing-it-out)\\n\\n- [Filter k](#filter-k)', metadata={'changefreq': 'weekly', 'description': 'Chroma is a database for building AI applications with embeddings.', 'language': 'en', 'loc': 'https://python.langchain.com/docs/integrations/retrievers/self_query/chroma_self_query', 'priority': '0.5', 'source': 'https://python.langchain.com/docs/integrations/retrievers/self_query/chroma_self_query', 'title': 'Chroma | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How to load an existed Chroma DB store?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "\n",
    "# After the retriever fetches documents, this\n",
    "# function formats them in a string to present for the LLM\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = (\n",
    "            f\"<document index='{i}'>\\n\"\n",
    "            f\"<source>{doc.metadata.get('source')}</source>\\n\"\n",
    "            f\"<doc_content>{doc.page_content}</doc_content>\\n\"\n",
    "            \"</document>\"\n",
    "        )\n",
    "        formatted_docs.append(doc_string)\n",
    "    formatted_str = \"\\n\".join(formatted_docs)\n",
    "    return f\"<documents>\\n{formatted_str}\\n</documents>\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You will be provided with documents to expand your knowledge about the topic:\"\n",
    "            \"\\n{context}\\n\"\n",
    "            \"Carefully respond to the user query.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0.6, model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "response_generator = (prompt | llm | StrOutputParser()).with_config(\n",
    "    run_name=\"GenerateResponse\",\n",
    ")\n",
    "\n",
    "# This is the final response chain.\n",
    "# It fetches the \"question\" key from the input dict,\n",
    "# passes it to the retriever, then formats as a string.\n",
    "\n",
    "chain = (\n",
    "    RunnableAssign(\n",
    "        {\n",
    "            \"context\": (itemgetter(\"question\") | retriever | format_docs).with_config(\n",
    "                run_name=\"FormatDocs\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    # The \"RunnableAssign\" above returns a dict with keys\n",
    "    # question (from the original input) and\n",
    "    # context: the string-formatted docs.\n",
    "    # This is passed to the response_generator above\n",
    "    | response_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'get_or_create_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mChroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:126\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m(\n\u001b[1;32m    127\u001b[0m     name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    128\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_relevance_score_fn \u001b[38;5;241m=\u001b[39m relevance_score_fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'get_or_create_collection'"
     ]
    }
   ],
   "source": [
    "Chroma(client=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `RunnableAssign` class is not mentioned in the provided documents. It's possible that it may be a custom or specific class within a particular codebase or framework that is not covered in the provided documentation.\n",
      "\n",
      "If you have specific details or context about the `RunnableAssign` class, please provide more information so that I can assist you further. Alternatively, if it is a specific class within a framework or library, you may want to refer to the official documentation or resources related to that framework for more detailed information.\n"
     ]
    }
   ],
   "source": [
    "q = '''What is RunnableAssign?\n",
    "'''\n",
    "r = chain.invoke({\"question\": q})\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"langchain.schema.runnable.base.RunnableBinding ‚Äî ü¶úüîó LangChain 0.0.337\\n\\nAPI\\n\\nExperimental\\n\\nPython Docs\\n\\nToggle Menu\\n\\nPrevUp\\nNext\\n\\nLangChain 0.0.337\\n\\nlangchain.schema.runnable.base.RunnableBinding\\n\\nlangchain.schema.runnable.base.RunnableBinding¬∂\\n\\nclass langchain.schema.runnable.base.RunnableBinding[source]¬∂\\nBases: RunnableBindingBase[Input, Output]\\nA runnable that delegates calls to another runnable with a set of kwargs.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\n\\nparam bound: langchain.schema.runnable.base.Runnable[langchain.schema.runnable.utils.Input, langchain.schema.runnable.utils.Output] [Required]¬∂\\n\\nparam config: langchain.schema.runnable.config.RunnableConfig [Optional]¬∂\\n\\nparam config_factories: List[Callable[[langchain.schema.runnable.config.RunnableConfig], langchain.schema.runnable.config.RunnableConfig]] [Optional]¬∂\\n\\nparam custom_input_type: Optional[Any] = None¬∂\\n\\nparam custom_output_type: Optional[Any] = None¬∂\\n\\nparam kwargs: Mapping[str, Any] [Optional]¬∂\\n\\nasync abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí List[Output]¬∂\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nasync ainvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí Output¬∂\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nasync astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí AsyncIterator[Output]¬∂\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nasync astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) ‚Üí Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]¬∂\\nStream all output from a runnable, as reported to the callback system.\\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\\nOutput is streamed as Log objects, which include a list of\\njsonpatch ops that describe how the state of the run has changed in each\\nstep, and the final state of the run.\\nThe jsonpatch ops can be applied in order to construct state.\\n\\nasync atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí AsyncIterator[Output]¬∂\\nDefault implementation of atransform, which buffers input and calls astream.\\nSubclasses should override this method if they can start producing output while\\ninput is still being generated.\\n\\nbatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí List[Output]¬∂\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nbind(**kwargs: Any) ‚Üí Runnable[Input, Output][source]¬∂\\nBind arguments to a Runnable, returning a new Runnable.\\n\\nconfig_schema(*, include: Optional[Sequence[str]] = None) ‚Üí Type[BaseModel]¬∂\\nThe type of config this runnable accepts specified as a pydantic model.\\nTo mark a field as configurable, see the configurable_fields\\nand configurable_alternatives methods.\\n\\nParameters\\ninclude ‚Äì A list of fields to include in the config schema.\\n\\nReturns\\nA pydantic model that can be used to validate config.\\n\\nconfigurable_alternatives(which: ConfigurableField, default_key: str = 'default', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]) ‚Üí RunnableSerializable[Input, Output]¬∂\\n\\nconfigurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) ‚Üí RunnableSerializable[Input, Output]¬∂\\n\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) ‚Üí Model¬∂\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‚Äòallow‚Äô was set since it adds all passed values\\n\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) ‚Üí Model¬∂\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\n\\nParameters\\n\\ninclude ‚Äì fields to include in new model\\nexclude ‚Äì fields to exclude from new model, as with values this takes precedence over include\\nupdate ‚Äì values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep ‚Äì set to True to make a deep copy of the model\\n\\nReturns\\nnew model instance\\n\\ndict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) ‚Üí DictStrAny¬∂\\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\\n\\nclassmethod from_orm(obj: Any) ‚Üí Model¬∂\\n\\nget_input_schema(config: Optional[RunnableConfig] = None) ‚Üí Type[BaseModel]¬∂\\nGet a pydantic model that can be used to validate input to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic input schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an input schema for a specific configuration.\\n\\nParameters\\nconfig ‚Äì A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate input.\\n\\nclassmethod get_lc_namespace() ‚Üí List[str]¬∂\\nGet the namespace of the langchain object.\\nFor example, if the class is langchain.llms.openai.OpenAI, then the\\nnamespace is [‚Äúlangchain‚Äù, ‚Äúllms‚Äù, ‚Äúopenai‚Äù]\\n\\nget_output_schema(config: Optional[RunnableConfig] = None) ‚Üí Type[BaseModel]¬∂\\nGet a pydantic model that can be used to validate output to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic output schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an output schema for a specific configuration.\\n\\nParameters\\nconfig ‚Äì A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate output.\\n\\ninvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí Output¬∂\\nTransform a single input into an output. Override to implement.\\n\\nParameters\\n\\ninput ‚Äì The input to the runnable.\\nconfig ‚Äì A config to use when invoking the runnable.\\nThe config supports standard keys like ‚Äòtags‚Äô, ‚Äòmetadata‚Äô for tracing\\npurposes, ‚Äòmax_concurrency‚Äô for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details.\\n\\nReturns\\nThe output of the runnable.\\n\\nclassmethod is_lc_serializable() ‚Üí bool¬∂\\nIs this class serializable?\\n\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) ‚Üí unicode¬∂\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\n\\nclassmethod lc_id() ‚Üí List[str]¬∂\\nA unique identifier for this class for serialization purposes.\\nThe unique identifier is a list of strings that describes the path\\nto the object.\\n\\nmap() ‚Üí Runnable[List[Input], List[Output]]¬∂\\nReturn a new Runnable that maps a list of inputs to a list of outputs,\\nby calling invoke() with each input.\\n\\nclassmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) ‚Üí Model¬∂\\n\\nclassmethod parse_obj(obj: Any) ‚Üí Model¬∂\\n\\nclassmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) ‚Üí Model¬∂\\n\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') ‚Üí DictStrAny¬∂\\n\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) ‚Üí unicode¬∂\\n\\nstream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí Iterator[Output]¬∂\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nto_json() ‚Üí Union[SerializedConstructor, SerializedNotImplemented]¬∂\\n\\nto_json_not_implemented() ‚Üí SerializedNotImplemented¬∂\\n\\ntransform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí Iterator[Output]¬∂\\nDefault implementation of transform, which buffers input and then calls stream.\\nSubclasses should override this method if they can start producing output while\\ninput is still being generated.\\n\\nclassmethod update_forward_refs(**localns: Any) ‚Üí None¬∂\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\n\\nclassmethod validate(value: Any) ‚Üí Model¬∂\\n\\nwith_config(config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí Runnable[Input, Output][source]¬∂\\nBind config to a Runnable, returning a new Runnable.\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (<class 'Exception'>,)) ‚Üí RunnableWithFallbacksT[Input, Output]¬∂\\nAdd fallbacks to a runnable, returning a new Runnable.\\n\\nParameters\\n\\nfallbacks ‚Äì A sequence of runnables to try if the original runnable fails.\\nexceptions_to_handle ‚Äì A tuple of exception types to handle.\\n\\nReturns\\nA new Runnable that will try the original runnable, and then each\\nfallback in order, upon failures.\\n\\nwith_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None) ‚Üí Runnable[Input, Output][source]¬∂\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the runnable starts running, with the Run object.\\non_end: Called after the runnable finishes running, with the Run object.\\non_error: Called if the runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nwith_retry(**kwargs: Any) ‚Üí Runnable[Input, Output][source]¬∂\\nCreate a new Runnable that retries the original runnable on exceptions.\\n\\nParameters\\n\\nretry_if_exception_type ‚Äì A tuple of exception types to retry on\\nwait_exponential_jitter ‚Äì Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt ‚Äì The maximum number of attempts to make before giving up\\n\\nReturns\\nA new Runnable that retries the original runnable on exceptions.\\n\\nwith_types(input_type: Optional[Union[Type[Input], BaseModel]] = None, output_type: Optional[Union[Type[Output], BaseModel]] = None) ‚Üí Runnable[Input, Output][source]¬∂\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nproperty InputType: Type[langchain.schema.runnable.utils.Input]¬∂\\nThe type of input this runnable accepts specified as a type annotation.\\n\\nproperty OutputType: Type[langchain.schema.runnable.utils.Output]¬∂\\nThe type of output this runnable produces specified as a type annotation.\\n\\nproperty config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec]¬∂\\nList configurable fields for this runnable.\\n\\nproperty input_schema: Type[pydantic.main.BaseModel]¬∂\\nThe type of input this runnable accepts specified as a pydantic model.\\n\\nproperty lc_attributes: Dict¬∂\\nList of attribute names that should be included in the serialized kwargs.\\nThese attributes must be accepted by the constructor.\\n\\nproperty lc_secrets: Dict[str, str]¬∂\\nA map of constructor argument names to secret ids.\\n\\nFor example,{‚Äúopenai_api_key‚Äù: ‚ÄúOPENAI_API_KEY‚Äù}\\n\\nproperty output_schema: Type[pydantic.main.BaseModel]¬∂\\nThe type of output this runnable produces specified as a pydantic model.\\n\\n            ¬© 2023, Harrison Chase.\\n          Last updated on Nov 17, 2023.\\n          Show this page source\", metadata={'changefreq': '', 'description': '', 'language': 'en', 'loc': '', 'priority': '', 'source': 'https://api.python.langchain.com/en/latest/schema.runnable/langchain.schema.runnable.base.RunnableBinding.html', 'title': 'langchain.schema.runnable.base.RunnableBinding ‚Äî ü¶úüîó LangChain 0.0.337'}),\n",
       " Document(page_content=\"langchain.schema.runnable.configurable.RunnableConfigurableAlternatives ‚Äî ü¶úüîó LangChain 0.0.337\\n\\nAPI\\n\\nExperimental\\n\\nPython Docs\\n\\nToggle Menu\\n\\nPrevUp\\nNext\\n\\nLangChain 0.0.337\\n\\nlangchain.schema.runnable.configurable.RunnableConfigurableAlternatives\\n\\nlangchain.schema.runnable.configurable.RunnableConfigurableAlternatives¬∂\\n\\nclass langchain.schema.runnable.configurable.RunnableConfigurableAlternatives[source]¬∂\\nBases: DynamicRunnable[Input, Output]\\nA Runnable that can be dynamically configured.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\n\\nparam alternatives: Dict[str, Union[langchain.schema.runnable.base.Runnable[langchain.schema.runnable.utils.Input, langchain.schema.runnable.utils.Output], Callable[[], langchain.schema.runnable.base.Runnable[langchain.schema.runnable.utils.Input, langchain.schema.runnable.utils.Output]]]] [Required]¬∂\\n\\nparam default: langchain.schema.runnable.base.RunnableSerializable[langchain.schema.runnable.utils.Input, langchain.schema.runnable.utils.Output] [Required]¬∂\\n\\nparam default_key: str = 'default'¬∂\\n\\nparam which: langchain.schema.runnable.utils.ConfigurableField [Required]¬∂\\n\\nasync abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí List[Output]¬∂\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nasync ainvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí Output¬∂\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nasync astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí AsyncIterator[Output]¬∂\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nasync astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) ‚Üí Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]¬∂\\nStream all output from a runnable, as reported to the callback system.\\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\\nOutput is streamed as Log objects, which include a list of\\njsonpatch ops that describe how the state of the run has changed in each\\nstep, and the final state of the run.\\nThe jsonpatch ops can be applied in order to construct state.\\n\\nasync atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí AsyncIterator[Output]¬∂\\nDefault implementation of atransform, which buffers input and calls astream.\\nSubclasses should override this method if they can start producing output while\\ninput is still being generated.\\n\\nbatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) ‚Üí List[Output]¬∂\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying runnable uses an API which supports a batch mode.\\n\\nbind(**kwargs: Any) ‚Üí Runnable[Input, Output]¬∂\\nBind arguments to a Runnable, returning a new Runnable.\\n\\nconfig_schema(*, include: Optional[Sequence[str]] = None) ‚Üí Type[BaseModel]¬∂\\nThe type of config this runnable accepts specified as a pydantic model.\\nTo mark a field as configurable, see the configurable_fields\\nand configurable_alternatives methods.\\n\\nParameters\\ninclude ‚Äì A list of fields to include in the config schema.\\n\\nReturns\\nA pydantic model that can be used to validate config.\\n\\nconfigurable_alternatives(which: ConfigurableField, default_key: str = 'default', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]) ‚Üí RunnableSerializable[Input, Output]¬∂\\n\\nconfigurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) ‚Üí RunnableSerializable[Input, Output][source]¬∂\\n\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) ‚Üí Model¬∂\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‚Äòallow‚Äô was set since it adds all passed values\\n\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) ‚Üí Model¬∂\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\n\\nParameters\\n\\ninclude ‚Äì fields to include in new model\\nexclude ‚Äì fields to exclude from new model, as with values this takes precedence over include\\nupdate ‚Äì values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep ‚Äì set to True to make a deep copy of the model\\n\\nReturns\\nnew model instance\\n\\ndict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) ‚Üí DictStrAny¬∂\\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\\n\\nclassmethod from_orm(obj: Any) ‚Üí Model¬∂\\n\\nget_input_schema(config: Optional[RunnableConfig] = None) ‚Üí Type[BaseModel]¬∂\\nGet a pydantic model that can be used to validate input to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic input schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an input schema for a specific configuration.\\n\\nParameters\\nconfig ‚Äì A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate input.\\n\\nclassmethod get_lc_namespace() ‚Üí List[str]¬∂\\nGet the namespace of the langchain object.\\nFor example, if the class is langchain.llms.openai.OpenAI, then the\\nnamespace is [‚Äúlangchain‚Äù, ‚Äúllms‚Äù, ‚Äúopenai‚Äù]\\n\\nget_output_schema(config: Optional[RunnableConfig] = None) ‚Üí Type[BaseModel]¬∂\\nGet a pydantic model that can be used to validate output to the runnable.\\nRunnables that leverage the configurable_fields and configurable_alternatives\\nmethods will have a dynamic output schema that depends on which\\nconfiguration the runnable is invoked with.\\nThis method allows to get an output schema for a specific configuration.\\n\\nParameters\\nconfig ‚Äì A config to use when generating the schema.\\n\\nReturns\\nA pydantic model that can be used to validate output.\\n\\ninvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí Output¬∂\\nTransform a single input into an output. Override to implement.\\n\\nParameters\\n\\ninput ‚Äì The input to the runnable.\\nconfig ‚Äì A config to use when invoking the runnable.\\nThe config supports standard keys like ‚Äòtags‚Äô, ‚Äòmetadata‚Äô for tracing\\npurposes, ‚Äòmax_concurrency‚Äô for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details.\\n\\nReturns\\nThe output of the runnable.\\n\\nclassmethod is_lc_serializable() ‚Üí bool¬∂\\nIs this class serializable?\\n\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) ‚Üí unicode¬∂\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\n\\nclassmethod lc_id() ‚Üí List[str]¬∂\\nA unique identifier for this class for serialization purposes.\\nThe unique identifier is a list of strings that describes the path\\nto the object.\\n\\nmap() ‚Üí Runnable[List[Input], List[Output]]¬∂\\nReturn a new Runnable that maps a list of inputs to a list of outputs,\\nby calling invoke() with each input.\\n\\nclassmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) ‚Üí Model¬∂\\n\\nclassmethod parse_obj(obj: Any) ‚Üí Model¬∂\\n\\nclassmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) ‚Üí Model¬∂\\n\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') ‚Üí DictStrAny¬∂\\n\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) ‚Üí unicode¬∂\\n\\nstream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí Iterator[Output]¬∂\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nto_json() ‚Üí Union[SerializedConstructor, SerializedNotImplemented]¬∂\\n\\nto_json_not_implemented() ‚Üí SerializedNotImplemented¬∂\\n\\ntransform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) ‚Üí Iterator[Output]¬∂\\nDefault implementation of transform, which buffers input and then calls stream.\\nSubclasses should override this method if they can start producing output while\\ninput is still being generated.\\n\\nclassmethod update_forward_refs(**localns: Any) ‚Üí None¬∂\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\n\\nclassmethod validate(value: Any) ‚Üí Model¬∂\\n\\nwith_config(config: Optional[RunnableConfig] = None, **kwargs: Any) ‚Üí Runnable[Input, Output]¬∂\\nBind config to a Runnable, returning a new Runnable.\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (<class 'Exception'>,)) ‚Üí RunnableWithFallbacksT[Input, Output]¬∂\\nAdd fallbacks to a runnable, returning a new Runnable.\\n\\nParameters\\n\\nfallbacks ‚Äì A sequence of runnables to try if the original runnable fails.\\nexceptions_to_handle ‚Äì A tuple of exception types to handle.\\n\\nReturns\\nA new Runnable that will try the original runnable, and then each\\nfallback in order, upon failures.\\n\\nwith_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None) ‚Üí Runnable[Input, Output]¬∂\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the runnable starts running, with the Run object.\\non_end: Called after the runnable finishes running, with the Run object.\\non_error: Called if the runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nwith_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (<class 'Exception'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) ‚Üí Runnable[Input, Output]¬∂\\nCreate a new Runnable that retries the original runnable on exceptions.\\n\\nParameters\\n\\nretry_if_exception_type ‚Äì A tuple of exception types to retry on\\nwait_exponential_jitter ‚Äì Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt ‚Äì The maximum number of attempts to make before giving up\\n\\nReturns\\nA new Runnable that retries the original runnable on exceptions.\\n\\nwith_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None) ‚Üí Runnable[Input, Output]¬∂\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nproperty InputType: Type[langchain.schema.runnable.utils.Input]¬∂\\nThe type of input this runnable accepts specified as a type annotation.\\n\\nproperty OutputType: Type[langchain.schema.runnable.utils.Output]¬∂\\nThe type of output this runnable produces specified as a type annotation.\\n\\nproperty config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec]¬∂\\nList configurable fields for this runnable.\\n\\nproperty input_schema: Type[pydantic.main.BaseModel]¬∂\\nThe type of input this runnable accepts specified as a pydantic model.\\n\\nproperty lc_attributes: Dict¬∂\\nList of attribute names that should be included in the serialized kwargs.\\nThese attributes must be accepted by the constructor.\\n\\nproperty lc_secrets: Dict[str, str]¬∂\\nA map of constructor argument names to secret ids.\\n\\nFor example,{‚Äúopenai_api_key‚Äù: ‚ÄúOPENAI_API_KEY‚Äù}\\n\\nproperty output_schema: Type[pydantic.main.BaseModel]¬∂\\nThe type of output this runnable produces specified as a pydantic model.\\n\\n            ¬© 2023, Harrison Chase.\\n          Last updated on Nov 17, 2023.\\n          Show this page source\", metadata={'changefreq': '', 'description': '', 'language': 'en', 'loc': '', 'priority': '', 'source': 'https://api.python.langchain.com/en/latest/schema.runnable/langchain.schema.runnable.configurable.RunnableConfigurableAlternatives.html', 'title': 'langchain.schema.runnable.configurable.RunnableConfigurableAlternatives ‚Äî ü¶úüîó LangChain 0.0.337'}),\n",
       " Document(page_content='langchain.schema.runnable.base.coerce_to_runnable ‚Äî ü¶úüîó LangChain 0.0.337\\n\\nAPI\\n\\nExperimental\\n\\nPython Docs\\n\\nToggle Menu\\n\\nPrevUp\\nNext\\n\\nLangChain 0.0.337\\n\\nlangchain.schema.runnable.base.coerce_to_runnable\\n\\nlangchain.schema.runnable.base.coerce_to_runnable¬∂\\n\\nlangchain.schema.runnable.base.coerce_to_runnable(thing: Union[Runnable[Input, Output], Callable[[Input], Output], Callable[[Input], Awaitable[Output]], Callable[[Iterator[Input]], Iterator[Output]], Callable[[AsyncIterator[Input]], AsyncIterator[Output]], Mapping[str, Any]]) ‚Üí Runnable[Input, Output][source]¬∂\\nCoerce a runnable-like object into a Runnable.\\n\\nParameters\\nthing ‚Äì A runnable-like object.\\n\\nReturns\\nA Runnable.\\n\\n            ¬© 2023, Harrison Chase.\\n          Last updated on Nov 17, 2023.\\n          Show this page source', metadata={'changefreq': '', 'description': '', 'language': 'en', 'loc': '', 'priority': '', 'source': 'https://api.python.langchain.com/en/latest/schema.runnable/langchain.schema.runnable.base.coerce_to_runnable.html', 'title': 'langchain.schema.runnable.base.coerce_to_runnable ‚Äî ü¶úüîó LangChain 0.0.337'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''What is Runnable Assign?\n",
    "'''\n",
    "retriever.invoke(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.client import Client\n",
    "from langchain_benchmarks.rag import get_eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "RAG_EVALUATION = get_eval_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithAuthError",
     "evalue": "Authentication failed for https://api.smith.langchain.com/datasets. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=LangChain+Docs+Q%26A', '{\"detail\":\"Invalid API key\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/utils.py:83\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=LangChain+Docs+Q%26A",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/client.py:411\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[0;34m(self, request_method, url, request_kwargs, stop_after_attempt, retry_on)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    409\u001b[0m     request_method, url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_kwargs\n\u001b[1;32m    410\u001b[0m )\n\u001b[0;32m--> 411\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/utils.py:85\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHTTPError\u001b[0m: [Errno 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=LangChain+Docs+Q%26A] {\"detail\":\"Invalid API key\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLangSmithAuthError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Generate a unique run ID for this experiment\u001b[39;00m\n\u001b[1;32m      4\u001b[0m run_uid \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex[:\u001b[38;5;241m6\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m test_run \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlangchain_docs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRAG_EVALUATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-2 qa-chain simple-index \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_uid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbasic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/client.py:2874\u001b[0m, in \u001b[0;36mClient.run_on_dataset\u001b[0;34m(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, input_mapper)\u001b[0m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe client.run_on_dataset function requires the langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage to run.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstall with pip install langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2873\u001b[0m     )\n\u001b[0;32m-> 2874\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/smith/evaluation/runner_utils.py:1234\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[0;34m(client, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     warn_deprecated(\n\u001b[1;32m   1227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1228\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following arguments are deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[1;32m   1233\u001b[0m client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m Client()\n\u001b[0;32m-> 1234\u001b[0m container \u001b[38;5;241m=\u001b[39m \u001b[43m_DatasetRunContainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concurrency_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1246\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1247\u001b[0m         _run_llm_or_chain(\n\u001b[1;32m   1248\u001b[0m             example,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(container\u001b[38;5;241m.\u001b[39mexamples, container\u001b[38;5;241m.\u001b[39mconfigs)\n\u001b[1;32m   1254\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/smith/evaluation/runner_utils.py:1072\u001b[0m, in \u001b[0;36m_DatasetRunContainer.prepare\u001b[0;34m(cls, client, dataset_name, llm_or_chain_factory, project_name, evaluation, tags, input_mapper, concurrency_level, project_metadata)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     project_metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1070\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _DatasetRunContainer:\n\u001b[1;32m   1071\u001b[0m     project_name \u001b[38;5;241m=\u001b[39m project_name \u001b[38;5;129;01mor\u001b[39;00m name_generation\u001b[38;5;241m.\u001b[39mrandom_name()\n\u001b[0;32m-> 1072\u001b[0m     wrapped_model, project, dataset, examples \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_eval_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m     wrapped_model \u001b[38;5;241m=\u001b[39m _wrap_in_chain_factory(llm_or_chain_factory)\n\u001b[1;32m   1081\u001b[0m     run_evaluators \u001b[38;5;241m=\u001b[39m _setup_evaluation(\n\u001b[1;32m   1082\u001b[0m         wrapped_model, examples, evaluation, dataset\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;129;01mor\u001b[39;00m DataType\u001b[38;5;241m.\u001b[39mkv\n\u001b[1;32m   1083\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/smith/evaluation/runner_utils.py:927\u001b[0m, in \u001b[0;36m_prepare_eval_run\u001b[0;34m(client, dataset_name, llm_or_chain_factory, project_name, project_metadata, tags)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_eval_run\u001b[39m(\n\u001b[1;32m    919\u001b[0m     client: Client,\n\u001b[1;32m    920\u001b[0m     dataset_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    924\u001b[0m     tags: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    925\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[MCF, TracerSession, Dataset, List[Example]]:\n\u001b[1;32m    926\u001b[0m     wrapped_model \u001b[38;5;241m=\u001b[39m _wrap_in_chain_factory(llm_or_chain_factory, dataset_name)\n\u001b[0;32m--> 927\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(client\u001b[38;5;241m.\u001b[39mlist_examples(dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid))\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m examples:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/utils.py:73\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/client.py:1554\u001b[0m, in \u001b[0;36mClient.read_dataset\u001b[0;34m(self, dataset_name, dataset_id)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide dataset_name or dataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1554\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/client.py:472\u001b[0m, in \u001b[0;36mClient._get_with_retries\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_with_retries\u001b[39m(\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, params: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    471\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheaders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langsmith/client.py:426\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[0;34m(self, request_method, url, request_kwargs, stop_after_attempt, retry_on)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithRateLimitError(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithAuthError(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthentication failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n",
      "\u001b[0;31mLangSmithAuthError\u001b[0m: Authentication failed for https://api.smith.langchain.com/datasets. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=LangChain+Docs+Q%26A', '{\"detail\":\"Invalid API key\"}')"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique run ID for this experiment\n",
    "run_uid = uuid.uuid4().hex[:6]\n",
    "\n",
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain simple-index {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
