{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'developer-portal'...\n",
      "remote: Enumerating objects: 5258, done.\u001b[K\n",
      "remote: Counting objects: 100% (1774/1774), done.\u001b[K\n",
      "remote: Compressing objects: 100% (726/726), done.\u001b[K\n",
      "remote: Total 5258 (delta 1202), reused 1416 (delta 1021), pack-reused 3484\u001b[K\n",
      "Receiving objects: 100% (5258/5258), 79.68 MiB | 14.07 MiB/s, done.\n",
      "Resolving deltas: 100% (3385/3385), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/supervisely/developer-portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "docs_path = \"developer-portal\"\n",
    "\n",
    "loader = DirectoryLoader(docs_path, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-base\",\n",
    "    model_kwargs={\"device\": 0},  # Comment out to use CPU\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./supervisely-dev-portal-db\",\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='**Optional fields and loading** These fields are optional and are not needed when loading the project. The server can automatically fill in these fields while project is loading.\\n\\n* `id` - unique identifier of the current object\\n* `classId` - unique class identifier of the current object\\n* `labelerLogin` - string - the name of user who created the current figure\\n* `createdAt` - string - date and time of figure creation\\n* `updatedAt` - string - date and time of the last figure update\\n\\nMain idea of `key` fields and `id` you can see below in [Key id map file](point-clouds.md#key-id-map-file) section.\\n\\n**Fields definitions:**\\n\\n* `description` - string - (optional) - this field is used to store the text to assign to the sequence.\\n* `key` - string, unique key for a given sequence (used in key\\\\_id\\\\_map.json to get the sequence ID)\\n* `tags` - list of strings that will be interpreted as point cloud tags\\n* `objects` - list of objects that may be present on the dataset\\n* `geometryType` - \"cuboid\\\\_3d\" or other 3D geometry - class shape\\n\\n**Fields definitions for `objects` field:**\\n\\n* `key` - string - unique key for a given object (used in key\\\\_id\\\\_map.json)\\n* `classTitle` - string - the title of a class. It\\'s used to identify the class shape from the `meta.json` file\\n* `tags` - list of strings that will be interpreted as object tags (can be empty)\\n\\n**Fields description for `figures` field:**\\n\\n* `key` - string - unique key for a given figure (used in key\\\\_id\\\\_map.json)\\n* `objectKey` - string - unique key to link figure to object (used in key\\\\_id\\\\_map.json)\\n* `geometryType` - \"cuboid\\\\_3d\" or other 3D geometry -class shape\\n* `geometry` - geometry of the object\\n\\n**Description for `geometry` field (cuboid\\\\_3d):**\\n\\n* `position` 3D vector of box center coordinates:\\n  * **x** - forward in the direction of the object\\n  * **y** - left\\n  * **z** - up\\n* `dimensions` is a 3D vector that scales a cuboid from its local center along x,y,z:\\n  * **x** - width\\n  * **y** - length\\n  * **z** - height\\n* `rotation` is a 3D Vector that rotates a cuboid along an axis in world space:\\n  * **x** - pitch\\n  * **y** - roll\\n  * **z** - yaw (direction)\\n\\nRotation values bound inside \\\\[**-pi** ; **pi** ] When `yaw = 0` box direction will be strict `+y`\\n\\n## Key id map file\\n\\nThe basic idea behind key-id-map is that it maps the unique identifiers of entities from Supervisely to local entities keys. It is needed for such local data manipulations as cloning entities and reassigning relations between them. Examples of entities in `key_id_map.json`: datasets (videos), tags, objects, figures.\\n\\n```\\n{\\n    \"tags\": {},\\n    \"objects\": {\\n        \"198f727d40c749eebcacc4aed299b39a\": 20520\\n    },\\n    \"figures\": {\\n        \"65f21690780e43b49863c3cbd07eab3a\": 503130811\\n    },\\n    \"videos\": {\\n        \"e9f0a3ae21be41d08eec166d454562be\": 42656\\n    }\\n}\\n```\\n\\n* `objects` - dictionary, where the key is a unique string, generated inside Supervisely environment to set mapping of current object in annotation, and values are unique integer ID related to the current object\\n* `figures` - dictionary, where the key is a unique string, generated inside Supervisely environment to set mapping of object on current frame in annotation, and values are unique integer ID related to the current frame\\n* `videos` - dictionary, where the key is unique string, generated inside Supervisely environment to set mapping of dataset in annotation, and value is a unique integer ID related to the current sequence\\n* `tags` - dictionary, where the keys are unique strings, generated inside Supervisely environment to set mapping of tag on current frame in annotation, and values are a unique integer ID related to the current tag\\n* **Key** - [generated by python3 function `uuid.uuid4().hex`](https://docs.python.org/3/library/uuid.html#uuid.uuid4). The unique string. All key values and id\\'s should be unique inside single project and can not be shared between frames\\\\sequences.\\n* **Value** - returned by server integer identifier while uploading object / figure / sequence / tag\\n\\n## Format of frame\\\\_pointcloud\\\\_map.json\\n\\nThis file stores mapping between point cloud files and annotation frames in the correct order.\\n\\n```\\n{\\n    \"0\" : \"frame1.pcd\",\\n    \"1\" : \"frame2.pcd\",\\n    \"2\" : \"frame3.pcd\" \\n}\\n```\\n\\n**Keys** - frame order number\\\\\\n**Values** - point cloud name (with extension)\\n\\n## Photo context image annotation file', metadata={'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-clouds.md'}),\n",
       "  0.8130197212662698),\n",
       " (Document(page_content='# Point Clouds\\n\\n<figure><img src=\"https://github.com/supervisely/developer-portal/raw/main/.gitbook/assets/3d_pointclouds_interface.png\" alt=\"\"><figcaption><p>3D Point Cloud labeling interface</p></figcaption></figure>\\n\\n## Project Structure Example\\n\\n```\\n<PROJECT_NAME>  \\n├── key_id_map.json (optional)              \\n├── meta.json     \\n├── <DATASET_NAME_1>                         \\n│ ├── pointcloud                    \\n│ │   ├── scene_1.pcd           \\n│ │   ├── scene_2.pcd   \\n│ │   └── ...                \\n│ ├── related_images (optional)               \\n│ │   ├── scene_1_pcd               \\n│ │   │ ├── scene_1_cam0.png       \\n│ │   │ ├── scene_1_cam0.png.json  \\n│ │   │ ├── scene_1_cam1.png       \\n│ │   │ ├── scene_1_cam1.png.json  \\n│ │   │ └── ... \\n│ │   ├── scene_2_pcd               \\n│ │   │ ├── scene_2_cam0.png       \\n│ │   │ ├── scene_2_cam0.png.json  \\n│ │   │ ├── scene_2_cam1.png       \\n│ │   │ ├── scene_2_cam1.png.json  \\n│ │   │ └── ... \\n│ │   └── ...      \\n│ └── ann\\n│     ├── scene_1.pcd.json\\n│     ├── scene_2.pcd.json\\n│     └── ...     \\n├── <DATASET_NAME_2>                     \\n│ ├── pointcloud                    \\n│ │   ├── scene_1.pcd\\n│ │   └── ...                \\n│ ├── related_images (optional)               \\n│ │   ├── scene_1_pcd               \\n│ │   │ ├── scene_1_cam0.png       \\n│ │   │ ├── scene_1_cam0.png.json  \\n│ │   │ ├── scene_1_cam1.png       \\n│ │   │ ├── scene_1_cam1.png.json  \\n│ │   │ └── ... \\n│ │   └── ...      \\n│ └── ann\\n│     ├── scene_1.pcd.json\\n│     └── ...                      \\n└── <DATASET_NAME_...>                       \\n```\\n\\n## Main concepts\\n\\n**Point cloud Project**\\n\\nPoint cloud Project consists of one or many datasets of point clouds.\\n\\nIt also includes Sensor fusion feature that supports video camera sensor in the Labeling Tool UI.\\n\\n**Project Meta (meta.json)**\\n\\nProject Meta contains the essential information about the project - Classes and Tags. These are defined project-wide and can be used for labeling in every dataset inside the current project.\\n\\n**Datasets (\\\\<DATASET\\\\_NAME\\\\_1>, \\\\<DATASET\\\\_NAME\\\\_2>, ...)**\\n\\nDatasets are the second level folders inside the project, they host a subsets of point cloud scenes, related photo context (images) and annotations.\\n\\n**Items/Point clouds (pointcloud)**\\n\\nEvery `.pcd` file in a sequence has to be stored inside a `pointcloud` folder of datasets.\\n\\n| Key | Value                                                     |\\n| --- | --------------------------------------------------------- |\\n| x   | The x coordinate of the point.                            |\\n| y   | The y coordinate of the point.                            |\\n| z   | The z coordinate of the point.                            |\\n| r   | The red color channel component. An 8-bit value (0-255).  |\\n| g   | The green color channel component. An 8-bit value (0-255) |\\n| b   | The blue color channel component. An 8-bit value (0-255)  |\\n\\nAll of the positional coordinates (x, y, z) are in meters. Supervisely supports all PCD encodings: ASCII, binary, binary\\\\_compressed.\\n\\nThe PCD file format description can be found [here](https://pointclouds.org/documentation/tutorials/pcd\\\\_file\\\\_format.html)\\n\\n**Items Annotations (ann)**\\n\\nPoint cloud Annotations refer to each point cloud and contains information about labels on the point clouds in the datasets.\\n\\nA dataset has a list of `objects` that can be shared between some of point clouds.\\n\\nThe list of `objects` is defined for the entire dataset, even if the object\\'s figure occurs in only one point cloud.\\n\\n`Figures` represents individual labels, attached to one single frame and its object.\\n\\n```\\n{\\n    \"description\": \"\",\\n    \"key\": \"e9f0a3ae21be41d08eec166d454562be\",\\n    \"tags\": [],\\n    \"objects\": [\\n        {\\n            \"key\": \"ecb975d70735486b90fe4fdd2be77e3b\",\\n            \"classTitle\": \"Car\",\\n            \"tags\": [],\\n            \"labelerLogin\": \"admin\",\\n            \"updatedAt\": \"2022-05-04T00:32:30.872Z\",\\n            \"createdAt\": \"2022-05-04T00:32:30.872Z\"\\n        }\\n    ],\\n    \"figures\": [\\n        {\\n            \"key\": \"abbaec8785c1468585f6210c62bb2374\",\\n            \"objectKey\": \"ecb975d70735486b90fe4fdd2be77e3b\",\\n            \"geometryType\": \"cuboid_3d\",\\n            \"geometry\": {\\n                \"position\": {\\n                    \"x\": 58.756710052490234,\\n                    \"y\": 4.623323917388916,\\n                    \"z\": -0.4174150526523591\\n                },\\n                \"rotation\": {\\n                    \"x\": 0,\\n                    \"y\": 0,\\n                    \"z\": -1.77\\n                },\\n                \"dimensions\": {\\n                    \"x\": 1.59,\\n                    \"y\": 4.28,\\n                    \"z\": 1.45\\n                }\\n            },\\n            \"labelerLogin\": \"admin\",\\n            \"updatedAt\": \"2022-05-04T00:32:37.432Z\",\\n            \"createdAt\": \"2022-05-04T00:32:37.432Z\"\\n        }\\n    ]\\n}\\n```', metadata={'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-clouds.md'}),\n",
       "  0.7925628000374159),\n",
       " (Document(page_content='**Output:**\\n\\n```python\\n# Point cloud name: pcd_0.pcd\\n```\\n\\n### Get information about context images\\n\\nGet information about related context images. For example it can be a photo from front/back cameras of a vehicle.\\n\\n**Source code:**\\n\\n```python\\nimg_infos = api.pointcloud.get_list_related_images(pcd_info.id)\\nimg_info = img_infos[0]\\nprint(img_info)\\n```\\n\\n**Output:**\\n\\n```python\\n{\\'pathOriginal\\': \\'/h5un6l2bnaz1vj8a9qgms4-public/images/original/S/j/hJ/PwhtY7x4zRQ5jvNETPgFMtjJ9bDOMkjJelovMYLJJL2wxsGS9dvSjQC428ORi2qIFYg4u1gbiN7DsRIfO3JVBEt0xRgNc0vm3n2DTv8UiV9HXoaCp0Fy4IoObKMg.png\\',\\n \\'id\\': 473302,\\n \\'entityId\\': 17557533,\\n \\'createdAt\\': \\'2023-01-09T08:50:33.225Z\\',\\n \\'updatedAt\\': \\'2023-01-09T08:50:33.225Z\\',\\n \\'meta\\': {\\'deviceId\\': \\'cam_2\\'},\\n \\'fileMeta\\': {\\'mime\\': \\'image/png\\',\\n  \\'size\\': 893783,\\n  \\'width\\': 1224,\\n  \\'height\\': 370},\\n \\'hash\\': \\'vxA+emfDNUkFP9P6oitMB5Q0rMlnskmV2jvcf47OjGU=\\',\\n \\'link\\': None,\\n \\'preview\\': \\'/previews/q/ext:jpeg/resize:fill:50:0:0/q:50/plain/h5un6l2bnaz1vj8a9qgms4-public/images/original/S/j/hJ/PwhtY7x4zRQ5jvNETPgFMtjJ9bDOMkjJelovMYLJJL2wxsGS9dvSjQC428ORi2qIFYg4u1gbiN7DsRIfO3JVBEt0xRgNc0vm3n2DTv8UiV9HXoaCp0Fy4IoObKMg.png\\',\\n \\'fullStorageUrl\\': \\'https://dev.supervise.ly/h5un6l2bnaz1vj8a9qgms4-public/images/original/S/j/hJ/PwhtY7x4zRQ5jvNETPgFMtjJ9bDOMkjJelovMYLJJL2wxsGS9dvSjQC428ORi2qIFYg4u1gbiN7DsRIfO3JVBEt0xRgNc0vm3n2DTv8UiV9HXoaCp0Fy4IoObKMg.png\\',\\n \\'name\\': \\'img0.png\\'}\\n```\\n\\n### Get list of all point clouds in the dataset\\n\\nYou can list all point clouds in the dataset.\\n\\n**Source code:**\\n\\n```python\\npcd_infos = api.pointcloud.get_list(dataset.id)\\nprint(f\"Dataset contains {len(pcd_infos)} point clouds\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Dataset contains 3 point clouds\\n```\\n\\n## Download point clouds and context images from Supervisely\\n\\n### Download a point cloud\\n\\nDownload point cloud from Supervisely to local directory by id.\\n\\n**Source code:**\\n\\n```python\\nsave_path = \"src/output/pcd_0.pcd\"\\napi.pointcloud.download_path(pcd_info.id, save_path)\\nprint(f\"Point cloud has been successfully downloaded to \\'{save_path}\\'\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Point cloud has been successfully downloaded to \\'src/output/pcd_0.pcd\\'\\n```\\n\\n### Download a related context image\\n\\nDownload a related context image from Supervisely to local directory by image id.\\n\\n**Source code:**\\n\\n```python\\nsave_path = \"src/output/img_0.png\"\\nimg_info = api.pointcloud.get_list_related_images(pcd_info.id)[0]\\napi.pointcloud.download_related_image(img_info[\"id\"], save_path)\\nprint(f\"Context image has been successfully downloaded to \\'{save_path}\\'\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Context image has been successfully downloaded to \\'src/output/img_0.png\\'\\n```\\n\\n\\n\\n## Working with Point Cloud Episodes\\n\\nWorking with Point Cloud Episodes is similar, except the following:\\n\\n1. There is `api.pointcloud_episode` for working with episodes.\\n2. Create new projects with type `sly.ProjectType.POINT_CLOUD_EPISODES`.\\n3. Put the frame index in meta while uploading a pcd: `meta = {\"frame\": idx}`.\\n\\n**Note:** in Supervisely each episode is treated as a dataset. Therefore, create a separate dataset every time you want to add a new episode.\\n\\n### Create new project and dataset\\n\\nCreate new project.\\n\\n**Source code:**\\n\\n```python\\nproject = api.project.create(\\n    workspace_id,\\n    name=\"Point Cloud Episodes Tutorial\",\\n    type=sly.ProjectType.POINT_CLOUD_EPISODES,\\n    change_name_if_conflict=True,\\n)\\nprint(f\"Project ID: {project.id}\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Project ID: 16197\\n```\\n\\nCreate new dataset.\\n\\n**Source code:**\\n\\n```python\\ndataset = api.dataset.create(project.id, \"dataset_1\")\\nprint(f\"Dataset ID: {dataset.id}\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Dataset ID: 54539\\n```\\n\\n### Upload one point cloud to Supervisely.\\n\\n**Source code:**\\n\\n```python\\nmeta = {\"frame\": 0}  # \"frame\" is a required field for Episodes\\npcd_info = api.pointcloud_episode.upload_path(dataset.id, \"pcd_0.pcd\", \"src/input/pcd/000000.pcd\", meta=meta)\\nprint(f\\'Point cloud \"{pcd_info.name}\" (frame={meta[\"frame\"]}) uploaded to Supervisely\\')\\n```\\n\\n**Output:**\\n\\n```python\\n# Point cloud \"pcd_0.pcd\" (frame=0) uploaded to Supervisely\\n```\\n\\n### Upload entire point clouds episode to Supervisely platform.\\n\\n**Source code:**\\n\\n```python\\ndef read_cam_info(cam_info_file):\\n    with open(cam_info_file, \"r\") as f:\\n        cam_info = json.load(f)\\n    return cam_info\\n\\n\\n# 1. get paths\\ninput_path = \"src/input\"\\npcd_files = list(Path(f\"{input_path}/pcd\").glob(\"*.pcd\"))\\nimg_files = list(Path(f\"{input_path}/img\").glob(\"*.png\"))\\ncam_info_files = Path(f\"{input_path}/cam_info\").glob(\"*.json\")\\n\\n# 2. get names and metas\\npcd_metas = [{\"frame\": i} for i in range(len(pcd_files))]\\nimg_metas = [read_cam_info(cam_info_file) for cam_info_file in cam_info_files]\\npcd_names = list(map(os.path.basename, pcd_files))\\nimg_names = list(map(os.path.basename, img_files))', metadata={'source': 'developer-portal/getting-started/python-sdk-tutorials/point-clouds/point-clouds-and-episodes.md'}),\n",
       "  0.7812224528757108),\n",
       " (Document(page_content='#### `volumeMeta` fields description:\\n\\n* `ACS` - string - \"RAS\" or \"LPS\" - name of type of [Anatomical coordinate system](https://www.slicer.org/wiki/Coordinate\\\\_systems#Anatomical\\\\_coordinate\\\\_system) i.e. RAS means is Right-Anterior-Superior\\n\\n```\\n╔════════╦════════════╗\\n║ Common ║ Anatomical ║\\n╠════════╬════════════╣\\n║ Left   ║ Left       ║\\n║ Right  ║ Right      ║\\n║ Up     ║ Superior   ║\\n║ Down   ║ Inferior   ║\\n║ Front  ║ Anterior   ║\\n║ Back   ║ Posterior  ║\\n╚════════╩════════════╝\\n```\\n\\n\\n\\n* `intensity` - `{\"min\": int, \"max\": int}` - intensity range. Depends on the device getting the data\\n* `windowWidth` - float - Specify a linear conversion. Window Width contains the width of the window\\n* `windowCenter` - float - Specify a linear conversion. Window Center contains the value that is the center of the window\\n* `channelsCount` - float - channel count of your image data. Default: 1\\n* `dimensionsIJK` - dict {\"x\": int, \"y\": int, \"z\": int} - dimensions of volume described as vector in [IJK notation](https://en.wikipedia.org/wiki/Unit\\\\_vector)\\n* `IJK2WorldMatrix` - matrix to transform coordinates from IJK to world (cartesian). See [here](https://www.slicer.org/wiki/Coordinate\\\\_systems#Image\\\\_transformation)\\n\\nGrayscale transformations to be applied to Pixel Data are defined by the equivalent of the Modality LUT and Rescale Intercept, Value of Interest Attributes, Photometric Interpretation and the equivalent of the Presentation LUT.\\n\\n`units = m*SV + b`\\n\\n* `rescaleSlope` - float - m in the equation specified by Rescale Intercept\\n* `rescaleIntercept` - float - The value \"b\" in the relationship between stored values (SV) in Pixel Data and the output units specified in Rescale Type.\\n\\n\\n#### `objects` fields description:\\n\\n* `key` - string - a unique identifier of given object represented as `UUID.hex` value (used in `key_id_map.json` to get the object ID)\\n* `classTitle` - string - the title of a class. It\\'s used to identify the class shape from the `meta.json` file\\n* `tags` - list of strings that will be interpreted as object tags\\n* `labelerLogin` - string - the name of the user that added this figure to the project\\n* `updatedAt` - string - the date and time when the `object` was updated (ISO 8601 format)\\n* `createdAt` - string - the date and time when the `object` was updated (ISO 8601 format)\\n\\n\\n\\n#### `planes` fields description:\\n\\n* `name` - string - the name of the plane, where the figures are placed. Can be [coronal, sagittal or axial](https://www.slicer.org/wiki/Coordinate\\\\_systems#Anatomical\\\\_coordinate\\\\_system)\\n\\n    ![Anatomical space](../../.gitbook/assets/body\\\\_planes.png)\\n\\n*   `normal` - dict with x, y, z as keys and 0/1 as values - normal is direction by axis, chosen according to plane name\\n\\n    * sagittal - x\\n    * coronal - y\\n    * axial - z\\n\\n    The value is binary `(int 0 or 1)` and one plane must be selected.\\n* `slices` - list of slices on the plane. Each list contain index and may contain figures.\\n\\n#### `slices` fields description:\\n\\n* `index` - int value of slice index\\n* `figures` - list of figures placed on slice. It can be [bitmap](objects.md#bitmap) or [rectangle](objects.md#rectangle).\\n\\n#### `spatialFigures` fields description\\nThis list contains 3D objects of type [Mask3D](objects.md#mask3d-3d-annotation)\\n\\n* `key` - string - unique key for a given figure (used in `key_id_map.json`)\\n* `objectKey` - string - unique key to link figure to object (used in `key_id_map.json`)\\n* `geometryType` - `mask_3d` or other 3D geometry-class shape\\n* `geometry` - geometry of the object\\n\\n\\n## NRRD files in `mask` folder\\n\\nThese files contain geometry for 3D annotation objects, every file name must be the same as figure key to which it belongs.\\n\\nExample: \\n\\n`/project_name/dataset_name/mask/CTChest.nrrd/daff638a423a4bcfa34eb12e42243a87.nrrd` connected with spatial figure `\"key\": \"daff638a423a4bcfa34eb12e42243a87\"`\\n\\nDefinitions for its fields can be found [here](https://teem.sourceforge.net/nrrd/format.html)\\n\\n\\n## Key id map file\\n\\n`/project_name/key_id_map.json` file is optional. It is created when annotating the volume inside Supervisely interface and sets the correspondence between the unique identifiers of the object and the volume on which the figure is located. If you annotate manually, you do not need to create this file. This will not affect the work being done.\\n\\nJSON file format of `key_id_map.json`:\\n\\n```json\\n{\\n    \"tags\": {},\\n    \"objects\": {\\n        \"198f727d40c749eebcacc4aed299b39a\": 20520\\n    },\\n    \"figures\": {\\n        \"65f21690780e43b49863c3cbd07eab3a\": 503130811\\n    },\\n    \"videos\": {\\n        \"e9f0a3ae21be41d08eec166d454562be\": 42656\\n    }\\n}\\n```', metadata={'source': 'developer-portal/api-references/supervisely-annotation-json-format/volumes-annotation.md'}),\n",
       "  0.771951199874527)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_relevance_scores(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='**Optional fields and loading** These fields are optional and are not needed when loading the project. The server can automatically fill in these fields while project is loading.\\n\\n* `id` - unique identifier of the current object\\n* `classId` - unique class identifier of the current object\\n* `labelerLogin` - string - the name of user who created the current figure\\n* `createdAt` - string - date and time of figure creation\\n* `updatedAt` - string - date and time of the last figure update\\n\\nMain idea of `key` fields and `id` you can see below in [Key id map file](point-clouds.md#key-id-map-file) section.\\n\\n**Fields definitions:**\\n\\n* `description` - string - (optional) - this field is used to store the text to assign to the sequence.\\n* `key` - string, unique key for a given sequence (used in key\\\\_id\\\\_map.json to get the sequence ID)\\n* `tags` - list of strings that will be interpreted as point cloud tags\\n* `objects` - list of objects that may be present on the dataset\\n* `geometryType` - \"cuboid\\\\_3d\" or other 3D geometry - class shape\\n\\n**Fields definitions for `objects` field:**\\n\\n* `key` - string - unique key for a given object (used in key\\\\_id\\\\_map.json)\\n* `classTitle` - string - the title of a class. It\\'s used to identify the class shape from the `meta.json` file\\n* `tags` - list of strings that will be interpreted as object tags (can be empty)\\n\\n**Fields description for `figures` field:**\\n\\n* `key` - string - unique key for a given figure (used in key\\\\_id\\\\_map.json)\\n* `objectKey` - string - unique key to link figure to object (used in key\\\\_id\\\\_map.json)\\n* `geometryType` - \"cuboid\\\\_3d\" or other 3D geometry -class shape\\n* `geometry` - geometry of the object\\n\\n**Description for `geometry` field (cuboid\\\\_3d):**\\n\\n* `position` 3D vector of box center coordinates:\\n  * **x** - forward in the direction of the object\\n  * **y** - left\\n  * **z** - up\\n* `dimensions` is a 3D vector that scales a cuboid from its local center along x,y,z:\\n  * **x** - width\\n  * **y** - length\\n  * **z** - height\\n* `rotation` is a 3D Vector that rotates a cuboid along an axis in world space:\\n  * **x** - pitch\\n  * **y** - roll\\n  * **z** - yaw (direction)\\n\\nRotation values bound inside \\\\[**-pi** ; **pi** ] When `yaw = 0` box direction will be strict `+y`\\n\\n## Key id map file\\n\\nThe basic idea behind key-id-map is that it maps the unique identifiers of entities from Supervisely to local entities keys. It is needed for such local data manipulations as cloning entities and reassigning relations between them. Examples of entities in `key_id_map.json`: datasets (videos), tags, objects, figures.\\n\\n```\\n{\\n    \"tags\": {},\\n    \"objects\": {\\n        \"198f727d40c749eebcacc4aed299b39a\": 20520\\n    },\\n    \"figures\": {\\n        \"65f21690780e43b49863c3cbd07eab3a\": 503130811\\n    },\\n    \"videos\": {\\n        \"e9f0a3ae21be41d08eec166d454562be\": 42656\\n    }\\n}\\n```\\n\\n* `objects` - dictionary, where the key is a unique string, generated inside Supervisely environment to set mapping of current object in annotation, and values are unique integer ID related to the current object\\n* `figures` - dictionary, where the key is a unique string, generated inside Supervisely environment to set mapping of object on current frame in annotation, and values are unique integer ID related to the current frame\\n* `videos` - dictionary, where the key is unique string, generated inside Supervisely environment to set mapping of dataset in annotation, and value is a unique integer ID related to the current sequence\\n* `tags` - dictionary, where the keys are unique strings, generated inside Supervisely environment to set mapping of tag on current frame in annotation, and values are a unique integer ID related to the current tag\\n* **Key** - [generated by python3 function `uuid.uuid4().hex`](https://docs.python.org/3/library/uuid.html#uuid.uuid4). The unique string. All key values and id\\'s should be unique inside single project and can not be shared between frames\\\\sequences.\\n* **Value** - returned by server integer identifier while uploading object / figure / sequence / tag\\n\\n## Format of frame\\\\_pointcloud\\\\_map.json\\n\\nThis file stores mapping between point cloud files and annotation frames in the correct order.\\n\\n```\\n{\\n    \"0\" : \"frame1.pcd\",\\n    \"1\" : \"frame2.pcd\",\\n    \"2\" : \"frame3.pcd\" \\n}\\n```\\n\\n**Keys** - frame order number\\\\\\n**Values** - point cloud name (with extension)\\n\\n## Photo context image annotation file', metadata={'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-clouds.md'}),\n",
       " Document(page_content='# Point Clouds\\n\\n<figure><img src=\"https://github.com/supervisely/developer-portal/raw/main/.gitbook/assets/3d_pointclouds_interface.png\" alt=\"\"><figcaption><p>3D Point Cloud labeling interface</p></figcaption></figure>\\n\\n## Project Structure Example\\n\\n```\\n<PROJECT_NAME>  \\n├── key_id_map.json (optional)              \\n├── meta.json     \\n├── <DATASET_NAME_1>                         \\n│ ├── pointcloud                    \\n│ │   ├── scene_1.pcd           \\n│ │   ├── scene_2.pcd   \\n│ │   └── ...                \\n│ ├── related_images (optional)               \\n│ │   ├── scene_1_pcd               \\n│ │   │ ├── scene_1_cam0.png       \\n│ │   │ ├── scene_1_cam0.png.json  \\n│ │   │ ├── scene_1_cam1.png       \\n│ │   │ ├── scene_1_cam1.png.json  \\n│ │   │ └── ... \\n│ │   ├── scene_2_pcd               \\n│ │   │ ├── scene_2_cam0.png       \\n│ │   │ ├── scene_2_cam0.png.json  \\n│ │   │ ├── scene_2_cam1.png       \\n│ │   │ ├── scene_2_cam1.png.json  \\n│ │   │ └── ... \\n│ │   └── ...      \\n│ └── ann\\n│     ├── scene_1.pcd.json\\n│     ├── scene_2.pcd.json\\n│     └── ...     \\n├── <DATASET_NAME_2>                     \\n│ ├── pointcloud                    \\n│ │   ├── scene_1.pcd\\n│ │   └── ...                \\n│ ├── related_images (optional)               \\n│ │   ├── scene_1_pcd               \\n│ │   │ ├── scene_1_cam0.png       \\n│ │   │ ├── scene_1_cam0.png.json  \\n│ │   │ ├── scene_1_cam1.png       \\n│ │   │ ├── scene_1_cam1.png.json  \\n│ │   │ └── ... \\n│ │   └── ...      \\n│ └── ann\\n│     ├── scene_1.pcd.json\\n│     └── ...                      \\n└── <DATASET_NAME_...>                       \\n```\\n\\n## Main concepts\\n\\n**Point cloud Project**\\n\\nPoint cloud Project consists of one or many datasets of point clouds.\\n\\nIt also includes Sensor fusion feature that supports video camera sensor in the Labeling Tool UI.\\n\\n**Project Meta (meta.json)**\\n\\nProject Meta contains the essential information about the project - Classes and Tags. These are defined project-wide and can be used for labeling in every dataset inside the current project.\\n\\n**Datasets (\\\\<DATASET\\\\_NAME\\\\_1>, \\\\<DATASET\\\\_NAME\\\\_2>, ...)**\\n\\nDatasets are the second level folders inside the project, they host a subsets of point cloud scenes, related photo context (images) and annotations.\\n\\n**Items/Point clouds (pointcloud)**\\n\\nEvery `.pcd` file in a sequence has to be stored inside a `pointcloud` folder of datasets.\\n\\n| Key | Value                                                     |\\n| --- | --------------------------------------------------------- |\\n| x   | The x coordinate of the point.                            |\\n| y   | The y coordinate of the point.                            |\\n| z   | The z coordinate of the point.                            |\\n| r   | The red color channel component. An 8-bit value (0-255).  |\\n| g   | The green color channel component. An 8-bit value (0-255) |\\n| b   | The blue color channel component. An 8-bit value (0-255)  |\\n\\nAll of the positional coordinates (x, y, z) are in meters. Supervisely supports all PCD encodings: ASCII, binary, binary\\\\_compressed.\\n\\nThe PCD file format description can be found [here](https://pointclouds.org/documentation/tutorials/pcd\\\\_file\\\\_format.html)\\n\\n**Items Annotations (ann)**\\n\\nPoint cloud Annotations refer to each point cloud and contains information about labels on the point clouds in the datasets.\\n\\nA dataset has a list of `objects` that can be shared between some of point clouds.\\n\\nThe list of `objects` is defined for the entire dataset, even if the object\\'s figure occurs in only one point cloud.\\n\\n`Figures` represents individual labels, attached to one single frame and its object.\\n\\n```\\n{\\n    \"description\": \"\",\\n    \"key\": \"e9f0a3ae21be41d08eec166d454562be\",\\n    \"tags\": [],\\n    \"objects\": [\\n        {\\n            \"key\": \"ecb975d70735486b90fe4fdd2be77e3b\",\\n            \"classTitle\": \"Car\",\\n            \"tags\": [],\\n            \"labelerLogin\": \"admin\",\\n            \"updatedAt\": \"2022-05-04T00:32:30.872Z\",\\n            \"createdAt\": \"2022-05-04T00:32:30.872Z\"\\n        }\\n    ],\\n    \"figures\": [\\n        {\\n            \"key\": \"abbaec8785c1468585f6210c62bb2374\",\\n            \"objectKey\": \"ecb975d70735486b90fe4fdd2be77e3b\",\\n            \"geometryType\": \"cuboid_3d\",\\n            \"geometry\": {\\n                \"position\": {\\n                    \"x\": 58.756710052490234,\\n                    \"y\": 4.623323917388916,\\n                    \"z\": -0.4174150526523591\\n                },\\n                \"rotation\": {\\n                    \"x\": 0,\\n                    \"y\": 0,\\n                    \"z\": -1.77\\n                },\\n                \"dimensions\": {\\n                    \"x\": 1.59,\\n                    \"y\": 4.28,\\n                    \"z\": 1.45\\n                }\\n            },\\n            \"labelerLogin\": \"admin\",\\n            \"updatedAt\": \"2022-05-04T00:32:37.432Z\",\\n            \"createdAt\": \"2022-05-04T00:32:37.432Z\"\\n        }\\n    ]\\n}\\n```', metadata={'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-clouds.md'}),\n",
       " Document(page_content='**Output:**\\n\\n```python\\n# Point cloud name: pcd_0.pcd\\n```\\n\\n### Get information about context images\\n\\nGet information about related context images. For example it can be a photo from front/back cameras of a vehicle.\\n\\n**Source code:**\\n\\n```python\\nimg_infos = api.pointcloud.get_list_related_images(pcd_info.id)\\nimg_info = img_infos[0]\\nprint(img_info)\\n```\\n\\n**Output:**\\n\\n```python\\n{\\'pathOriginal\\': \\'/h5un6l2bnaz1vj8a9qgms4-public/images/original/S/j/hJ/PwhtY7x4zRQ5jvNETPgFMtjJ9bDOMkjJelovMYLJJL2wxsGS9dvSjQC428ORi2qIFYg4u1gbiN7DsRIfO3JVBEt0xRgNc0vm3n2DTv8UiV9HXoaCp0Fy4IoObKMg.png\\',\\n \\'id\\': 473302,\\n \\'entityId\\': 17557533,\\n \\'createdAt\\': \\'2023-01-09T08:50:33.225Z\\',\\n \\'updatedAt\\': \\'2023-01-09T08:50:33.225Z\\',\\n \\'meta\\': {\\'deviceId\\': \\'cam_2\\'},\\n \\'fileMeta\\': {\\'mime\\': \\'image/png\\',\\n  \\'size\\': 893783,\\n  \\'width\\': 1224,\\n  \\'height\\': 370},\\n \\'hash\\': \\'vxA+emfDNUkFP9P6oitMB5Q0rMlnskmV2jvcf47OjGU=\\',\\n \\'link\\': None,\\n \\'preview\\': \\'/previews/q/ext:jpeg/resize:fill:50:0:0/q:50/plain/h5un6l2bnaz1vj8a9qgms4-public/images/original/S/j/hJ/PwhtY7x4zRQ5jvNETPgFMtjJ9bDOMkjJelovMYLJJL2wxsGS9dvSjQC428ORi2qIFYg4u1gbiN7DsRIfO3JVBEt0xRgNc0vm3n2DTv8UiV9HXoaCp0Fy4IoObKMg.png\\',\\n \\'fullStorageUrl\\': \\'https://dev.supervise.ly/h5un6l2bnaz1vj8a9qgms4-public/images/original/S/j/hJ/PwhtY7x4zRQ5jvNETPgFMtjJ9bDOMkjJelovMYLJJL2wxsGS9dvSjQC428ORi2qIFYg4u1gbiN7DsRIfO3JVBEt0xRgNc0vm3n2DTv8UiV9HXoaCp0Fy4IoObKMg.png\\',\\n \\'name\\': \\'img0.png\\'}\\n```\\n\\n### Get list of all point clouds in the dataset\\n\\nYou can list all point clouds in the dataset.\\n\\n**Source code:**\\n\\n```python\\npcd_infos = api.pointcloud.get_list(dataset.id)\\nprint(f\"Dataset contains {len(pcd_infos)} point clouds\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Dataset contains 3 point clouds\\n```\\n\\n## Download point clouds and context images from Supervisely\\n\\n### Download a point cloud\\n\\nDownload point cloud from Supervisely to local directory by id.\\n\\n**Source code:**\\n\\n```python\\nsave_path = \"src/output/pcd_0.pcd\"\\napi.pointcloud.download_path(pcd_info.id, save_path)\\nprint(f\"Point cloud has been successfully downloaded to \\'{save_path}\\'\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Point cloud has been successfully downloaded to \\'src/output/pcd_0.pcd\\'\\n```\\n\\n### Download a related context image\\n\\nDownload a related context image from Supervisely to local directory by image id.\\n\\n**Source code:**\\n\\n```python\\nsave_path = \"src/output/img_0.png\"\\nimg_info = api.pointcloud.get_list_related_images(pcd_info.id)[0]\\napi.pointcloud.download_related_image(img_info[\"id\"], save_path)\\nprint(f\"Context image has been successfully downloaded to \\'{save_path}\\'\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Context image has been successfully downloaded to \\'src/output/img_0.png\\'\\n```\\n\\n\\n\\n## Working with Point Cloud Episodes\\n\\nWorking with Point Cloud Episodes is similar, except the following:\\n\\n1. There is `api.pointcloud_episode` for working with episodes.\\n2. Create new projects with type `sly.ProjectType.POINT_CLOUD_EPISODES`.\\n3. Put the frame index in meta while uploading a pcd: `meta = {\"frame\": idx}`.\\n\\n**Note:** in Supervisely each episode is treated as a dataset. Therefore, create a separate dataset every time you want to add a new episode.\\n\\n### Create new project and dataset\\n\\nCreate new project.\\n\\n**Source code:**\\n\\n```python\\nproject = api.project.create(\\n    workspace_id,\\n    name=\"Point Cloud Episodes Tutorial\",\\n    type=sly.ProjectType.POINT_CLOUD_EPISODES,\\n    change_name_if_conflict=True,\\n)\\nprint(f\"Project ID: {project.id}\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Project ID: 16197\\n```\\n\\nCreate new dataset.\\n\\n**Source code:**\\n\\n```python\\ndataset = api.dataset.create(project.id, \"dataset_1\")\\nprint(f\"Dataset ID: {dataset.id}\")\\n```\\n\\n**Output:**\\n\\n```python\\n# Dataset ID: 54539\\n```\\n\\n### Upload one point cloud to Supervisely.\\n\\n**Source code:**\\n\\n```python\\nmeta = {\"frame\": 0}  # \"frame\" is a required field for Episodes\\npcd_info = api.pointcloud_episode.upload_path(dataset.id, \"pcd_0.pcd\", \"src/input/pcd/000000.pcd\", meta=meta)\\nprint(f\\'Point cloud \"{pcd_info.name}\" (frame={meta[\"frame\"]}) uploaded to Supervisely\\')\\n```\\n\\n**Output:**\\n\\n```python\\n# Point cloud \"pcd_0.pcd\" (frame=0) uploaded to Supervisely\\n```\\n\\n### Upload entire point clouds episode to Supervisely platform.\\n\\n**Source code:**\\n\\n```python\\ndef read_cam_info(cam_info_file):\\n    with open(cam_info_file, \"r\") as f:\\n        cam_info = json.load(f)\\n    return cam_info\\n\\n\\n# 1. get paths\\ninput_path = \"src/input\"\\npcd_files = list(Path(f\"{input_path}/pcd\").glob(\"*.pcd\"))\\nimg_files = list(Path(f\"{input_path}/img\").glob(\"*.png\"))\\ncam_info_files = Path(f\"{input_path}/cam_info\").glob(\"*.json\")\\n\\n# 2. get names and metas\\npcd_metas = [{\"frame\": i} for i in range(len(pcd_files))]\\nimg_metas = [read_cam_info(cam_info_file) for cam_info_file in cam_info_files]\\npcd_names = list(map(os.path.basename, pcd_files))\\nimg_names = list(map(os.path.basename, img_files))', metadata={'source': 'developer-portal/getting-started/python-sdk-tutorials/point-clouds/point-clouds-and-episodes.md'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "\n",
    "# After the retriever fetches documents, this\n",
    "# function formats them in a string to present for the LLM\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = (\n",
    "            f\"<document index='{i}'>\\n\"\n",
    "            f\"<source>{doc.metadata.get('source')}</source>\\n\"\n",
    "            f\"<doc_content>{doc.page_content}</doc_content>\\n\"\n",
    "            \"</document>\"\n",
    "        )\n",
    "        formatted_docs.append(doc_string)\n",
    "    formatted_str = \"\\n\".join(formatted_docs)\n",
    "    return f\"<documents>\\n{formatted_str}\\n</documents>\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Here are some documents from Supervisely Developer docs to expand your knowledge about the topic:\"\n",
    "            \"\\n{context}\\n\"\n",
    "            \"Carefully respond to the user query using the provided documents. It there is no relevant information, mention it to the user and try to answer based on your knowledge.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0.6, model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "response_generator = (prompt | llm | StrOutputParser()).with_config(\n",
    "    run_name=\"GenerateResponse\",\n",
    ")\n",
    "\n",
    "# This is the final response chain.\n",
    "# It fetches the \"question\" key from the input dict,\n",
    "# passes it to the retriever, then formats as a string.\n",
    "\n",
    "chain = (\n",
    "    RunnableAssign(\n",
    "        {\n",
    "            \"context\": (itemgetter(\"question\") | retriever | format_docs).with_config(\n",
    "                run_name=\"FormatDocs\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    # The \"RunnableAssign\" above returns a dict with keys\n",
    "    # question (from the original input) and\n",
    "    # context: the string-formatted docs.\n",
    "    # This is passed to the response_generator above\n",
    "    | response_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "for step in chain.stream({\"question\": q}):\n",
    "    trace.append(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When loading a Point Cloud \".pcd\" file, the x, y, and z dimensions represent the spatial coordinates of each point in the point cloud. These coordinates define the position of the point in 3D space. The r, g, and b dimensions represent the color information for each point, with values ranging from 0 to 255 for the red, green, and blue color channels, respectively.\n",
      "\n",
      "Here's a breakdown of the dimensions:\n",
      "- **x**: The x coordinate of the point\n",
      "- **y**: The y coordinate of the point\n",
      "- **z**: The z coordinate of the point\n",
      "- **r**: The red color channel component (0-255)\n",
      "- **g**: The green color channel component (0-255)\n",
      "- **b**: The blue color channel component (0-255)\n",
      "\n",
      "These values collectively define the spatial position and color of each point in the point cloud.\n"
     ]
    }
   ],
   "source": [
    "q = '''When I load a Point Cloud \".pcd\" file, what will be x,y,z and other dimensions mean?'''\n",
    "r = chain.invoke({\"question\": q})\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r, file=open(\"output.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-clouds.md'},\n",
       " {'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-clouds.md'},\n",
       " {'source': 'developer-portal/getting-started/python-sdk-tutorials/point-clouds/point-clouds-and-episodes.md'},\n",
       " {'source': 'developer-portal/api-references/supervisely-annotation-json-format/volumes-annotation.md'},\n",
       " {'source': 'developer-portal/api-references/supervisely-annotation-json-format/point-cloud-episodes.md'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.metadata for x in vectorstore.similarity_search(q, k=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Point Clouds\\n\\n<figure><img src=\"https://github.com/supervisely/developer-portal/raw/main/.gitbook/assets/3d_pointclouds_interface.png\" alt=\"\"><figcaption><p>3D Point Cloud labeling interface</p></figcaption></figure>\\n\\n## Project Structure Example\\n\\n```\\n<PROJECT_NAME>  \\n├── key_id_map.json (optional)              \\n├── meta.json     \\n├── <DATASET_NAME_1>                         \\n│ ├── pointcloud                    \\n│ │   ├── scene_1.pcd           \\n│ │   ├── scene_2.pcd   \\n│ │   └── ...                \\n│ ├── related_images (optional)               \\n│ │   ├── scene_1_pcd               \\n│ │   │ ├── scene_1_cam0.png       \\n│ │   │ ├── scene_1_cam0.png.json  \\n│ │   │ ├── scene_1_cam1.png       \\n│ │   │ ├── scene_1_cam1.png.json  \\n│ │   │ └── ... \\n│ │   ├── scene_2_pcd               \\n│ │   │ ├── scene_2_cam0.png       \\n│ │   │ ├── scene_2_cam0.png.json  \\n│ │   │ ├── scene_2_cam1.png       \\n│ │   │ ├── scene_2_cam1.png.json  \\n│ │   │ └── ... \\n│ │   └── ...      \\n│ └── ann\\n│     ├── scene_1.pcd.json\\n│     ├── scene_2.pcd.json\\n│     └── ...     \\n├── <DATASET_NAME_2>                     \\n│ ├── pointcloud                    \\n│ │   ├── scene_1.pcd\\n│ │   └── ...                \\n│ ├── related_images (optional)               \\n│ │   ├── scene_1_pcd               \\n│ │   │ ├── scene_1_cam0.png       \\n│ │   │ ├── scene_1_cam0.png.json  \\n│ │   │ ├── scene_1_cam1.png       \\n│ │   │ ├── scene_1_cam1.png.json  \\n│ │   │ └── ... \\n│ │   └── ...      \\n│ └── ann\\n│     ├── scene_1.pcd.json\\n│     └── ...                      \\n└── <DATASET_NAME_...>                       \\n```\\n\\n## Main concepts\\n\\n**Point cloud Project**\\n\\nPoint cloud Project consists of one or many datasets of point clouds.\\n\\nIt also includes Sensor fusion feature that supports video camera sensor in the Labeling Tool UI.\\n\\n**Project Meta (meta.json)**\\n\\nProject Meta contains the essential information about the project - Classes and Tags. These are defined project-wide and can be used for labeling in every dataset inside the current project.\\n\\n**Datasets (\\\\<DATASET\\\\_NAME\\\\_1>, \\\\<DATASET\\\\_NAME\\\\_2>, ...)**\\n\\nDatasets are the second level folders inside the project, they host a subsets of point cloud scenes, related photo context (images) and annotations.\\n\\n**Items/Point clouds (pointcloud)**\\n\\nEvery `.pcd` file in a sequence has to be stored inside a `pointcloud` folder of datasets.\\n\\n| Key | Value                                                     |\\n| --- | --------------------------------------------------------- |\\n| x   | The x coordinate of the point.                            |\\n| y   | The y coordinate of the point.                            |\\n| z   | The z coordinate of the point.                            |\\n| r   | The red color channel component. An 8-bit value (0-255).  |\\n| g   | The green color channel component. An 8-bit value (0-255) |\\n| b   | The blue color channel component. An 8-bit value (0-255)  |\\n\\nAll of the positional coordinates (x, y, z) are in meters. Supervisely supports all PCD encodings: ASCII, binary, binary\\\\_compressed.\\n\\nThe PCD file format description can be found [here](https://pointclouds.org/documentation/tutorials/pcd\\\\_file\\\\_format.html)\\n\\n**Items Annotations (ann)**\\n\\nPoint cloud Annotations refer to each point cloud and contains information about labels on the point clouds in the datasets.\\n\\nA dataset has a list of `objects` that can be shared between some of point clouds.\\n\\nThe list of `objects` is defined for the entire dataset, even if the object\\'s figure occurs in only one point cloud.\\n\\n`Figures` represents individual labels, attached to one single frame and its object.\\n\\n```\\n{\\n    \"description\": \"\",\\n    \"key\": \"e9f0a3ae21be41d08eec166d454562be\",\\n    \"tags\": [],\\n    \"objects\": [\\n        {\\n            \"key\": \"ecb975d70735486b90fe4fdd2be77e3b\",\\n            \"classTitle\": \"Car\",\\n            \"tags\": [],\\n            \"labelerLogin\": \"admin\",\\n            \"updatedAt\": \"2022-05-04T00:32:30.872Z\",\\n            \"createdAt\": \"2022-05-04T00:32:30.872Z\"\\n        }\\n    ],\\n    \"figures\": [\\n        {\\n            \"key\": \"abbaec8785c1468585f6210c62bb2374\",\\n            \"objectKey\": \"ecb975d70735486b90fe4fdd2be77e3b\",\\n            \"geometryType\": \"cuboid_3d\",\\n            \"geometry\": {\\n                \"position\": {\\n                    \"x\": 58.756710052490234,\\n                    \"y\": 4.623323917388916,\\n                    \"z\": -0.4174150526523591\\n                },\\n                \"rotation\": {\\n                    \"x\": 0,\\n                    \"y\": 0,\\n                    \"z\": -1.77\\n                },\\n                \"dimensions\": {\\n                    \"x\": 1.59,\\n                    \"y\": 4.28,\\n                    \"z\": 1.45\\n                }\\n            },\\n            \"labelerLogin\": \"admin\",\\n            \"updatedAt\": \"2022-05-04T00:32:37.432Z\",\\n            \"createdAt\": \"2022-05-04T00:32:37.432Z\"\\n        }\\n    ]\\n}\\n```'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(q)[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.metadata for x in vectorstore.similarity_search(q, k=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"autonomous agents\"\n",
    "r = retriever.invoke(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n"
     ]
    }
   ],
   "source": [
    "print(r[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
